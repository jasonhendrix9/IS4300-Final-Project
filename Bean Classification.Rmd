---
title: "Classification of Beans"
author: "Jason Hendrix"
date: "5/4/2022"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, comment = NA, message = F, out.width= "150%", out.height = "150%")
```

# Introduction

```{r "Set Up"}
# Packages ----

# Install any packages that we will be using that aren't already installed
packages <- c('tidyverse', 'caret', 'corrplot', 'rpart', 'rpart.plot', 'scales', 'pcaPP')
install.packages(setdiff(packages, rownames(installed.packages())))
rm(packages)

# Load packages

library(tidyverse)
library(caret)
library(corrplot)
library(rpart)
library(rpart.plot)
library(scales)
library(pcaPP)

# Set seed ----

suppressWarnings(set.seed(2022, sample.kind = "Rounding")) # remove warning output

# Read in ----

# This is the URL for the data that will be analyzed

urlfile <- "https://raw.githubusercontent.com/jasonhendrix9/IS4300-Final-Project/main/Dry_Bean_Dataset.csv"

# Read the data into R

df <- read_csv(url(urlfile))
# Make Class a factor ----

df$Class <- factor(df$Class)

class_index <- which(names(df) == 'Class') # Store the index of Class

# Split into train and test ----

# There's not a ton of data, so let's just do a 50/50 split for train and test

testIndex <- createDataPartition(df$Class, times = 1, p = 0.5, list = FALSE)

train1 <- df[-testIndex,]
test1 <- df[testIndex,]

# Scale ----

# Since we will be using KNN, which uses Euclidean distance, let's scale our variables

train1[-class_index] <- scale(train1[-class_index])
test1[-class_index] <- scale(test1[-class_index])
```

In this analysis, we will be considering over 10,000 picture observations of 7 different bean types. Our goal is to produce a few machine learning models that use the characteristics of a bean to predict its type. We will then evaluate the performance of each model to determine which one maximizes accuracy when evaluated on a separate test set. The data set can be found [on GitHub](https://github.com/jasonhendrix9/IS4300-Final-Project), and it was originally pulled from the University of California, Irvine, which may be found [here](https://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset).

We begin by examining the 16 variables included in the data, which are defined as follows:

1. *Area*: the number of pixels inside the boundaries of the bean
2. *Perimeter*: length of the border of the bean
3. *Major Axis Length*: the length of the longest line whose ends touch the boundary of the bean
4. *Minor Axis Length*: the length of the longest line whose ends touch the boundary of the bean when the bean is placed perpendicular to the Major Axis
5. *Aspect Ratio*: $\frac{\text{Major Axis Length}}{\text{Minor Axis Length}}$
6. *Eccentricity*: how near the ellipse that fits the around bean is to a circle (a value of 1 indicates a perfect circle)
7. *Convex Area*: the area of the smallest convex polygon that completely contains the bean
8. *Equivalent Diameter*: diameter of the circle that has the same area as the bean
9. *Extent*: the number of pixels in the image divided by the Area of the bean
10. *Solidity*: $\frac{\text{Convex Area}}{\text{Area}}$
11. *Roundness*: $4\pi \cdot \frac{\text{Area}}{\text{Perimeter}}$
12. *Compactness*: $\frac{\text{Equivalent Diameter}}{\text{Major Axis Length}}$
13. *Shape Factor 1*: $\frac{\text{Major Axis Length}}{\text{Area}}$
14. *Shape Factor 2*: $\frac{\text{Minor Axis Length}}{\text{Area}}$
15. *Shape Factor 3*: $\frac{4}{\pi}\frac{\text{Area}}{(\text{Major Axis Length})^2}$
16. *Shape Factor 4*: $\frac{4}{\pi}\frac{\text{Area}}{(\text{Major Axis Length}) \cdot (\text{Minor Axis Length})}$

We also have a variable for the bean type:

17. *Class*: Bean type - Barbunya, Bombay, Cali, Dermason, Horoz, Seker, or Sira

# Visualizations

While we will begin with training our models using variables 1-16 to predict variable 17, we will select a few of these variables to create a few plots of Class. We will see the significance of the variables that we visualize here later when we train the models, in the Analysis section.

## Perimeter vs Minor Axis Length

We will first consider the Perimeter and Minor Axis Length variables.

```{r "Perimeter vs Minor Axis Length"}
# Scatter Plot

ggplot() +
  geom_point(data = train1, 
             mapping = aes(x = Perimeter, y = MinorAxisLength, col = Class), size = 0.1) + # color points by Class
  labs(title = 'Bean Class',
       subtitle = 'Using Perimeter and Minor Axis Length',
       x = 'Perimeter',
       y = 'Minor Axis Length',
       color = 'Bean Class') + # rename legend title
  theme_minimal()
```

If we switch our view from considering each bean and instead consider the density of the scatter plot, we can better visualize the clustering in our data.

```{r "Perimeter vs Minor Axis Length Density"}
# Density Plot

ggplot() +
  geom_density2d(data = train1, 
                 mapping = aes(Perimeter, MinorAxisLength, col = Class)) + # color lines by Class
  labs(title = 'Bean Class Density',
       subtitle = 'Using Perimeter and Minor Axis Length',
       x = 'Perimeter',
       y = 'Minor Axis Length',
       color = 'Bean Class') + # rename legend title
  theme_minimal()
```

While there is some overlap (especially among Barbunya and Cali beans), most of the bean types have semi-distinct representations as a combination of these two variables.

## Eccentricity vs Shape Factor 2

Now, we will generate the same plot types of the bean class using another couple of variables - Eccentricity and Shape Factor 2.

```{r "Eccentricity vs Shape Factor 2"}
# Scatter Plot

ggplot() +
  geom_point(data = train1, 
             mapping = aes(Eccentricity, ShapeFactor2, col = Class), size = 0.1) + # color points by Class
  labs(title = 'Bean Class',
       subtitle = 'Using Eccentricity and Shape Factor 2',
       x = 'Eccentricity',
       y = 'Shape Factor 2',
       color = 'Bean Class') + # rename legend
  theme_minimal()

# Density Plot

ggplot() +
  geom_density2d(data = train1, 
                 mapping = aes(Eccentricity, ShapeFactor2, col = Class)) + # color points by Class
  labs(title = 'Bean Plot by Class (Training Set)',
       subtitle = 'Using Eccentricity and Shape Factor 2',
       x = 'Eccentricity',
       y = 'Shape Factor 2',
       color = 'Bean Class') + # rename legend
  theme_minimal()
```

We again find that most of the bean types have nearly-distinct clusters, yet there is still much overlap between Barbunya and Cali beans.

# Analysis

We will now turn our attention to training models to classify the bean types. We begin with a decision tree, which will find and partition all relevant variables in a waterfall-type format. 

## Decision Tree

```{r "Decision Tree"}
# rpart Tree ----

# Let's train our model

tree1 <- rpart(Class ~ ., data = train1)

# Now let's plot our tree

suppressWarnings(rpart.plot(tree1)) # remove warning output

# Use the model to make predictions

y_pred_tree1 <- as.vector(predict(tree1, test1[-class_index], type = 'class'))

# Calculate the accuracy from the results

cm_tree <- table(Actual = test1$Class, Predicted = y_pred_tree1)
```

Using the decision tree above, we attain an `r 100 * round((cm_tree[1,1] + cm_tree[2,2] + cm_tree[3,3] + cm_tree[4,4] + cm_tree[5,5] + cm_tree[6,6] + cm_tree[7,7]) / sum(cm_tree), 3)`% accuracy in correctly predicting Class. The most significant variables to this model are Perimeter and Minor Axis Length, which we have already visualized in a previous section.

We transition now to using the K Nearest Neighbor approach, which will evaluate the Class of the nearest K points to each new point to determine the most likely Class for the new point. First, we must find an optimal value for K.

## Optimize K for Accuracy

```{r "Best K"}
# Best K ----

# F Score doesn't work since there are multiple factor levels to Class
# Let's just take a look and see what maximizes accuracy

kv <- seq(1, 101, 2)  # try neighbors 1, 3, 5, 7, 9, ..., 101

# Now wee will loop through all Ks in our list and get the most accurate one

bestK <- 1
bestK_accuracy <- 0
accuracies <- c()

for(i in kv)
{
  # Train the model
  
  knn_i <- knn3(Class ~ ., data = train1, k = i)
  
  # Make predictions
  
  y_pred_i <- predict(knn_i, test1, type = "class") %>%
    factor(levels = levels(train1$Class))
  
  # Calculate accuracy
  
  cm_i <- table(Actual = test1$Class, Predicted = y_pred_i)
  
  accuracy <- (cm_i[1,1] + cm_i[2,2] + cm_i[3,3] + cm_i[4,4] + cm_i[5,5] + cm_i[6,6] + cm_i[7,7]) / sum(cm_i)
  
  # If this is the best accuracy so far, store it
  
  if(accuracy > bestK_accuracy)
  {
    bestK <- i
    bestK_accuracy <- accuracy
  }
  
  # Keep a list of all accuracies so we can plot it later
  
  accuracies <- c(accuracies, accuracy)
}

# Plot the accuracies

ggplot() + 
  geom_line(mapping = aes(kv, accuracies)) +  # y = accuracy
  geom_vline(xintercept = bestK, # vertical line to mark the most accurate
             col = 'firebrick', 
             linetype = 'dashed') +
  geom_hline(yintercept = bestK_accuracy, # horizontal line to mark the highest accuracy
             col = 'firebrick', 
             linetype = 'dashed') +
  scale_x_continuous(breaks = c(seq(0, 100, 25), bestK)) + # label the vertical line with the value
  scale_y_continuous(labels = percent, # make the y axis percents
                     breaks = c(seq(0.90, 0.92, 0.005), round(bestK_accuracy, 3))) + # label the horizontal line with the accuracy
  labs(title = 'KNN Accuracy',
       subtitle = 'With Varying Levels of K',
       x = 'K', 
       y = 'Accuracy') +
  theme_minimal()

# Turns out the best K is 15

# Train a new model with K = 15

knn_best1 <- knn3(Class ~ ., data = train1, k = bestK)

# Make predictions

y_pred_knn_best1 <- predict(knn_best1, test1, type = "class") %>%
  factor(levels = levels(train1$Class))

# Calculate Accuracy

cm_knn_best <- table(Actual = test1$Class, Predicted = y_pred_knn_best1)
```

Our maximum accuracy is `r 100 * round((cm_knn_best[1,1] + cm_knn_best[2,2] + cm_knn_best[3,3] + cm_knn_best[4,4] + cm_knn_best[5,5] + cm_knn_best[6,6] + cm_knn_best[7,7]) / sum(cm_knn_best), 3)`% at K = `r bestK`, which is considerably better than the accuracy of the decision tree.

## KNN Performance

Since there were a few beans that were incorrectly classified by the KNN model, it might be worthwhile to consider where those beans fall on the two most important variables to the decision tree.

```{r "Performance Plot"}
# Evaluation Plots ----

# 2 Most Important Variables to the first tree
# Sort the variables in order of descending importance and then return the top 2

# names(head(sort(tree1$variable.importance, 
#                 decreasing = T), 2))

# Top two are Perimeter and MinorAxisLength

# Points that were incorrectly predicted

wrong1 <- cbind.data.frame(test1$Perimeter, test1$MinorAxisLength, test1$Class, y_pred_knn_best1) %>% # combine variables, actual results, and predicted results
  rename(Perimeter = 1, # the numbers represent the column number - for example, 1 = test1$Perimeter
         MinorAxisLength = 2,
         Class = 3,
         Predicted = 4) %>% 
  filter(Class != Predicted) # only the incorrect predictions

ggplot() +
  geom_density2d(data = train1, 
                 mapping = aes(Perimeter, MinorAxisLength, col = Class)) + # color lines by class
  geom_point(data = wrong1,
             mapping = aes(Perimeter, MinorAxisLength), # scatter plot
             size = 0.25) + # make points small
  labs(title = 'Incorrect Bean Classification',
       subtitle = 'Density is Training Data and Points are Incorrect Predictions',
       x = 'Perimeter',
       y = 'Minor Axis Length',
       color = 'Bean Class') + # rename legend title
  theme_minimal()
```

Most of these errors on the plot above are in the overlap of Dermason and Sira in Perimeter and Minor Axis Length.

## Correlation of Variables

While we have created a very accurate model using all of the variables, we should seek to reduce the number of dimensions, if possible. A large reduction in dimensionality is often worth a trade-off with a slight reduction in accuracy.

If two variables are highly correlated, they won't be independent. Since two dependent variables convey about the same information to the model, we will remove highly correlated variables and retrain the models. In the following chart, the size of a dot indicates the magnitude of the correlation between the variables that it represents, and a red dot indicates negative correlation and a blue dot indicates positive correlation.

```{r "Correlation 1"}
# Correlation ----

# If two variables are highly correlated, they won't be independent. Since two dependent variables convey about the same information in this case, we will remove highly correlated variables and retrain the models

# Simpler > Slightly more accurate

# Most of the variables are not normal, so we should probably use Kendall instead of Pearson correlation
# We could use cor(..., method = 'kendall'), but that is really slow - O(n^2)
# Instead, let's use cor.fk - O(n log n)

# Let's make a plot

corrplot(cor.fk(as.data.frame(df[-class_index])), # correlation matrix
         tl.srt = 45) # rotate top 45ยบ
```

Now, let's remove all of the variables that have at least 90% correlation with another variable.

```{r "Correlation 2"}
# Let's remove all variables that have |corr| > 90% with any other variable
# The findCorrelation function does that for us

df.clean <- df[-findCorrelation(cor.fk(as.data.frame(df[-class_index])))]

# Find which column is the class column again

class_index <- which(names(df.clean) == 'Class')

# Replot our correlations

corrplot(cor.fk(as.data.frame(df.clean[-class_index])), 
         tl.srt = 45)

# They look much cleaner!
```

This reduction makes the relationships appear much cleaner.

```{r "Reset"}
# Split into train and test ----

train <- df.clean[-testIndex,]
test <- df.clean[testIndex,]

# Scale ----

train[-class_index] <- scale(train[-class_index])
test[-class_index] <- scale(test[-class_index])
```

Now, we can retrain our models and evaluate impact to the accuracy for each.

# Decision Tree with Correlated Variables Removed

```{r "Decision Tree II"}
# rpart ----

# Train a new model

tree <- rpart(Class ~ ., data = train)

# Plot the tree

suppressWarnings(rpart.plot(tree))

# Make predictions on the test set

y_pred_tree <- as.vector(predict(tree, test[-class_index], type = 'class'))

# Calculate accuracy

cm_tree <- table(Actual = test$Class, Predicted = y_pred_tree)
```

After retraining the decision tree, we have a new accuracy of `r 100 * round((cm_tree[1,1] + cm_tree[2,2] + cm_tree[3,3] + cm_tree[4,4] + cm_tree[5,5] + cm_tree[6,6] + cm_tree[7,7]) / sum(cm_tree), 3)`% - we traded a 7-dimension reduction for less than a 1% penalty to accuracy than before. The most important variables have changed to Eccentricity and Shape Factor 2, which has been visualized in the Visualization section.

Now, we retrain the KNN model.

## KNN with Correlated Variables Removed

Let's re-evaluate the optimal value for K.

```{r "Best K II"}
# Best K ----

# F Score doesn't work since there are multiple outputs
# Let's just make a look and see what maximizes accuracy

kv <- seq(1, 101, 2)  # try neighbors 1, 3, 5, 7, 9, ..., 101

# Loop through the neighbors to find the K with the highest accuracy

bestK <- 1
bestK_accuracy <- 0
accuracies <- c()

for(i in kv)
{
  # Create the model
  
  knn_i <- knn3(Class ~ ., data = train, k = i)
  
  # Make predictions
  
  y_pred_i <- predict(knn_i, test, type = "class") %>%
    factor(levels = levels(train$Class))
  
  # Calculate Accuracy
  
  cm_i <- table(Actual = test$Class, Predicted = y_pred_i)
  
  accuracy <- (cm_i[1,1] + cm_i[2,2] + cm_i[3,3] + cm_i[4,4] + cm_i[5,5] + cm_i[6,6] + cm_i[7,7]) / sum(cm_i)
  
  # If the accuracy is the best so far, keep it
  
  if(accuracy > bestK_accuracy)
  {
    bestK <- i
    bestK_accuracy <- accuracy
  }
  
  # Store all accuracies to plot later
  
  accuracies <- c(accuracies, accuracy)
}

ggplot() + 
  geom_line(mapping = aes(kv, accuracies)) + # y = accuracy
  geom_vline(xintercept = bestK, # vertical line for the best K
             col = 'firebrick', 
             linetype = 'dashed') +
  geom_hline(yintercept = bestK_accuracy, # horizontal line for the highest accuracy
             col = 'firebrick', 
             linetype = 'dashed') +
  scale_x_continuous(breaks = c(seq(0, 100, 25), bestK)) + # label the best K
  scale_y_continuous(labels = percent, # make y axis be percents
                     breaks = c(seq(0.90, 0.92, 0.005), round(bestK_accuracy, 3))) + # label the highest accuracy
  labs(title = 'KNN Accuracy',
       subtitle = 'With Varying Levels of K',
       x = 'K', 
       y = 'Accuracy') +
  theme_minimal()

# Turns out the best K is 17

knn_best <- knn3(Class ~ ., data = train, k = bestK)

y_pred_knn_best <- predict(knn_best, test, type = "class") %>%
  factor(levels = levels(train$Class))

cm_knn_best <- table(Actual = test$Class, Predicted = y_pred_knn_best)

# Very little accuracy was lost, and we have a much smaller model!
```

The best K is `r bestK`, which yields an accuracy of `r 100 * round((cm_knn_best[1,1] + cm_knn_best[2,2] + cm_knn_best[3,3] + cm_knn_best[4,4] + cm_knn_best[5,5] + cm_knn_best[6,6] + cm_knn_best[7,7]) / sum(cm_knn_best), 3)`%, which again is a less than 1% reduction in accuracy than before.

# Conclusion

We have constructed and compared several machine learning models to classify beans. We found that for this data set, K Nearest Neighbors was the optimal algorithm - it classified the bean type with an accuracy of 92%. We have also shown that for an opportunity cost of less than 1% accuracy for both modelling methods, we could reduce the number of variables in each model by almost 50%, greatly reducing model complexity.